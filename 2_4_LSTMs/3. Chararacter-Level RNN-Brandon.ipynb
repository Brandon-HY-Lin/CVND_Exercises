{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data and Inspect Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(text) =1,985,223\n",
      "\n",
      "\n",
      "text[0:100]=\n",
      "Chapter 1\n",
      "\n",
      "\n",
      "Happy families are all alike; every un\n"
     ]
    }
   ],
   "source": [
    "print('len(text) ={:,}'.format(len(text)))\n",
    "print('\\n')\n",
    "print('text[0:100]=\\n{}'.format(text[0:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Text Encoder and Encode Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = tuple(set(text))\n",
    "idx2word = dict(enumerate(chars))\n",
    "word2idx = {word: idx for idx, word in idx2word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(word2idx)=83\n"
     ]
    }
   ],
   "source": [
    "print('len(word2idx)={}'.format(len(word2idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2idx={',': 0, 'l': 1, 'c': 2, 'w': 3, 'O': 4, 'x': 5, '(': 6, 'G': 7, '\"': 8, 'd': 9, 'v': 10, ':': 11, 'A': 12, 'B': 13, 'j': 14, 's': 15, 'F': 16, 'Y': 17, 'Q': 18, 'e': 19, 'o': 20, '`': 21, '/': 22, 'X': 23, 'u': 24, 'U': 25, '.': 26, '\\n': 27, 'M': 28, '8': 29, '5': 30, 'm': 31, ')': 32, 'z': 33, '0': 34, '*': 35, 'C': 36, 'h': 37, 'y': 38, '$': 39, '!': 40, '9': 41, '&': 42, 'g': 43, 'L': 44, 'H': 45, ' ': 46, '@': 47, 'S': 48, '?': 49, 'J': 50, 'k': 51, '3': 52, '7': 53, 'i': 54, 't': 55, 'q': 56, 'N': 57, 'r': 58, 'a': 59, \"'\": 60, 'D': 61, 'I': 62, '2': 63, 'V': 64, 'p': 65, '-': 66, ';': 67, 'n': 68, '_': 69, 'f': 70, '4': 71, '6': 72, 'W': 73, 'R': 74, 'T': 75, 'P': 76, 'b': 77, 'Z': 78, '%': 79, 'E': 80, 'K': 81, '1': 82}\n",
      "text[0:50]=\n",
      "Chapter 1\n",
      "\n",
      "\n",
      "Happy families are all alike; every un\n",
      "\n",
      "\n",
      "encoded[0:50]=\n",
      "[36, 37, 59, 65, 55, 19, 58, 46, 82, 27, 27, 27, 45, 59, 65, 65, 38, 46, 70, 59, 31, 54, 1, 54, 19, 15, 46, 59, 58, 19, 46, 59, 1, 1, 46, 59, 1, 54, 51, 19, 67, 46, 19, 10, 19, 58, 38, 46, 24, 68]\n"
     ]
    }
   ],
   "source": [
    "encoded = [word2idx[w] for w in text]\n",
    "\n",
    "print('word2idx={}'.format(word2idx))\n",
    "print('text[0:50]=\\n{}'.format(text[0:50]))\n",
    "print('\\n')\n",
    "print('encoded[0:50]=\\n{}'.format(encoded[0:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_batches(text, batch_size, seq_length):\n",
    "    n_seq = len(text) // (batch_size * seq_length)\n",
    "    text = np.array(text[:batch_size * seq_length * n_seq])\n",
    "    text = text.reshape((batch_size, -1))\n",
    "    \n",
    "    for i in range(0, text.shape[1], seq_length):\n",
    "        \n",
    "        x = text[:, i:i+seq_length]\n",
    "        y = np.zeros_like(x)\n",
    "\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], text[:, i+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], text[:, 0]\n",
    "#             print('IndexError: i={}'.format(i))\n",
    "\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Batch Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "\tseq 1\n",
      "\t\tx:\t[59 43 68 54 70 54  2 19 68 55 67 46 55 37 54 58 55 38 66 19]\n",
      "\t\ty:\t   [43 68 54 70 54  2 19 68 55 67 46 55 37 54 58 55 38 66 19 54]\n",
      "\t\tx_word:\t'agnificent; thirty-e'\n",
      "\t\ty_word:\t 'gnificent; thirty-ei'\n",
      "\tseq 2\n",
      "\t\tx:\t[19 46 55 37 59 68 51 19  9 46 44 19 10 54 68 46 59 68  9 46]\n",
      "\t\ty:\t   [46 55 37 59 68 51 19  9 46 44 19 10 54 68 46 59 68  9 46  3]\n",
      "\t\tx_word:\t'e thanked Levin and '\n",
      "\t\ty_word:\t ' thanked Levin and w'\n",
      "Batch 1:\n",
      "\tseq 0\n",
      "\t\tx:\t[31 54  1 54 19 15 46 59 58 19 46 59  1  1 46 59  1 54 51 19]\n",
      "\t\ty:\t   [54  1 54 19 15 46 59 58 19 46 59  1  1 46 59  1 54 51 19 67]\n",
      "\t\tx_word:\t'milies are all alike'\n",
      "\t\ty_word:\t 'ilies are all alike;'\n",
      "Batch 2:\n",
      "\tseq 0\n",
      "\t\tx:\t[67 46 19 10 19 58 38 46 24 68 37 59 65 65 38 46 70 59 31 54]\n",
      "\t\ty:\t   [46 19 10 19 58 38 46 24 68 37 59 65 65 38 46 70 59 31 54  1]\n",
      "\t\tx_word:\t'; every unhappy fami'\n",
      "\t\ty_word:\t ' every unhappy famil'\n",
      "\tseq 1\n",
      "\t\tx:\t[46 15 55 58 59 54 43 37 55 46 59  3 59 38  0 46 59 68  9 46]\n",
      "\t\ty:\t   [15 55 58 59 54 43 37 55 46 59  3 59 38  0 46 59 68  9 46 55]\n",
      "\t\tx_word:\t' straight away, and '\n",
      "\t\ty_word:\t 'straight away, and t'\n",
      "\tseq 2\n",
      "\t\tx:\t[68 46  3 19 68 55 46 54 68 55 20 46 55 37 19 46 51 54 55  2]\n",
      "\t\ty:\t   [46  3 19 68 55 46 54 68 55 20 46 55 37 19 46 51 54 55  2 37]\n",
      "\t\tx_word:\t'n went into the kitc'\n",
      "\t\ty_word:\t ' went into the kitch'\n"
     ]
    }
   ],
   "source": [
    "for index, (x, y) in enumerate(get_batches(encoded, 5, 20)):\n",
    "    if index == 3:\n",
    "        break\n",
    "        \n",
    "    print('Batch {}:'.format(index))\n",
    "        \n",
    "    for i_seq, (x_seq, y_seq) in enumerate(zip(x, y)):\n",
    "        \n",
    "        # Don't show newline because of readability\n",
    "        if word2idx['\\n'] in x_seq:\n",
    "            continue \n",
    "            \n",
    "        if i_seq == 3:\n",
    "            break\n",
    "            \n",
    "        print('\\tseq {}'.format(i_seq))\n",
    "        print('\\t\\tx:\\t{}'.format(x_seq))\n",
    "        print('\\t\\ty:\\t   {}'.format(y_seq))\n",
    "        \n",
    "        x_word = [idx2word[idx] for idx in x_seq]\n",
    "        y_word = [idx2word[idx] for idx in y_seq]\n",
    "        print('\\t\\tx_word:\\t\\'{}\\''.format(''.join(x_word)))\n",
    "        print('\\t\\ty_word:\\t \\'{}\\''.format(''.join(y_word)))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define One-Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, tokens, n_steps=100, n_hidden=256, n_layers=2,\n",
    "                        drop_prob=0.5, lr=0.001):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(set(self.chars)))\n",
    "        self.char2int = {c: i for i, c in self.int2char.items()}\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=len(self.chars),\n",
    "                           hidden_size=n_hidden,\n",
    "                           num_layers=n_layers,\n",
    "                           dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hc):\n",
    "        \n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        x = x.view(x.size()[0]*x.size()[1], self.n_hidden)\n",
    "        \n",
    "        ## TODO: put x through the fully-connected layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x, (h, c)\n",
    "    \n",
    "    \n",
    "    def predict(self, char, h=None, cuda=False, top_k=None):\n",
    "        \n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "        else:\n",
    "            self.cpu()\n",
    "            \n",
    "        if h is None:\n",
    "            h = self.init_hidden(1)\n",
    "            \n",
    "        x = np.array([[self.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(self.chars))\n",
    "        \n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "            \n",
    "        h = tuple([each.data for each in h])\n",
    "        out, h = self.forward(inputs, h)\n",
    "        \n",
    "        p = F.softmax(out, dim=1).data\n",
    "        \n",
    "        if cuda:\n",
    "            p = p.cpu()\n",
    "            \n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(self.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "            \n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "            \n",
    "        return self.int2char[char], h\n",
    "        \n",
    "   \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        \n",
    "        self.fc.bias.data.fill_(0)\n",
    "        \n",
    "        self.fc.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        return (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, \n",
    "          lr=0.001, clip=5, val_frac=0.1, cuda=False, print_every=10):\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    # define optimizer\n",
    "    opt = optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    # define loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training set and validation set\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    \n",
    "    n_chars = len(net.chars)\n",
    "    \n",
    "    # start training\n",
    "    for e in range(epochs):\n",
    "        counter = 0\n",
    "        \n",
    "        # reset weight\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        # get batch of encoded data\n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            \n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "            # move tensor from cpu to gpu\n",
    "            if cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                \n",
    "                \n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "                \n",
    "            net.zero_grad()\n",
    "    \n",
    "            output, h = net.forward(inputs, h)\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "                \n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                \n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    \n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if cuda:\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                        \n",
    "                        \n",
    "                    output, val_h = net.forward(inputs, val_h)\n",
    "                    \n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "                    \n",
    "                    val_losses.append(val_loss.item())\n",
    "                    \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
    "            inputs = inputs.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'net' in locals():\n",
    "    del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = CharRNN(chars, n_hidden=512, n_layers=2)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1... Step: 10... Loss: 3.3540... Val Loss: 3.3189\n",
      "Epoch: 1/1... Step: 20... Loss: 3.1834... Val Loss: 3.2033\n",
      "Epoch: 1/1... Step: 30... Loss: 3.0794... Val Loss: 3.0674\n",
      "Epoch: 1/1... Step: 40... Loss: 2.8841... Val Loss: 2.8981\n",
      "Epoch: 1/1... Step: 50... Loss: 2.7661... Val Loss: 2.7212\n",
      "Epoch: 1/1... Step: 60... Loss: 2.6040... Val Loss: 2.6159\n",
      "Epoch: 1/1... Step: 70... Loss: 2.5317... Val Loss: 2.5475\n",
      "Epoch: 1/1... Step: 80... Loss: 2.4691... Val Loss: 2.5008\n",
      "Epoch: 1/1... Step: 90... Loss: 2.4442... Val Loss: 2.4586\n",
      "Epoch: 1/1... Step: 100... Loss: 2.3950... Val Loss: 2.4215\n",
      "Epoch: 1/1... Step: 110... Loss: 2.3486... Val Loss: 2.3903\n",
      "Epoch: 1/1... Step: 120... Loss: 2.2833... Val Loss: 2.3604\n",
      "Epoch: 1/1... Step: 130... Loss: 2.3091... Val Loss: 2.3402\n",
      "Execution Time:0:00:41.695086\n",
      "Time per Epoch=0:00:41.695086\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "batch_size, seq_length = 128, 100\n",
    "epochs = 1\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "# you may change cuda to True if you plan on using a GPU!\n",
    "# also, if you do, please INCREASE the epochs to 25\n",
    "train(net, encoded, epochs=epochs, batch_size=batch_size, seq_length=seq_length, \n",
    "      lr=0.001, cuda=True, print_every=10)\n",
    "\n",
    "end = datetime.now()\n",
    "print('Execution Time:{}'.format(end - start))\n",
    "print('Time per Epoch={}'.format((end-start)/epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'lstm_{}_epochs.net'.format(epochs)\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "             'n_layers': net.n_layers,\n",
    "             'state_dict': net.state_dict(),\n",
    "             'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None, cuda=False):\n",
    "        \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = net.predict(ch, h, cuda=cuda, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = net.predict(chars[-1], h, cuda=cuda, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna A_ arahaverad o2lle, a_d aq o4 Cersas aq t_d.aqqor as qo2s t_qere a_d, aq qCts t_e qCer qaq o4 Cer as t4ares a_d.o2sqed qe Car wor dt_qq qCtq Ce so2qCe 4ortqCe CtsCe ald Ce Co2dd qCa_ qo satd Ct_g altd, satde asqeladt_g qCaq o4 Cased qCaq a_d.o4 a_er qooqC qCer qCerered a_d qCers, Cad qCe ald o4 a_d Caqe qoo_gU .\"Rell, Ctd so5 qCe wasq Ce satde_, a_de_gqtg qCes a_d al o4.ale a_d qCe -ero_ qCe qCe stqCt_g Cts Cas add a_q Cer ad qo qCer wtsq Ce as qCe qarer qCe..4ore Cas wor qCe.qCe wald sqere 4ro2d qo Ce Cer ow Cts qCe alo_qCer as o2q a_g qCe qCerede a_d a_ wo2ld o4 t_ qCows o2qCed Ce 5aqed qCe alde_d waqere qo cares t_qCe seredesqo_ as tqCe so5aq a_ qoo Cererere Caq o4 qCe qo co Co5ered, saded o_d ove qo Cars qo st_ge, a_d aq t5lt_g aqC qCe -ored qo Ce 5ere qCo2s was qCt_g.a_ ald as ares ove co2rq or qto_g qo qCer.CtqC as a_d qCe Cers qCe Ceradde_q wor t_ sCad asqo_qer qCaq.sore sa_ aq o4 Cerses t4 Cer asd a_d Cever ald Ce qto_ a_d qCtq aqre_ qo Cts qCaq a_ qCe Cere so4ellh sCe qord a_dts, Corserad qCe_ qCa_d qCtg sCe wo2d Co Cer qalt_q aq qt_g Cer.a_dtq qCer, aq o_ed.waqC a_ Ctq Careds qa qCa_g qCeres qe qCt_g wtqC Cer a_ert_g, a_d qCt_g qCe a_d qo_ t_ Cor so_qered Cer Cad dt5e_sqt_g qCaq.o2d o4 a_ wt5l Ce wer Ce qo_g a_q qeled ar o4er qCer qCer a_ a_d.arde_ o4 qo co2ll a_g qCaq Ce -aqt_g as ale ale qCe wor do5ed qCer Casser o4 Ce Ctq o4 o2 qo 5a_er o4 qCe sCers wo2d a_d o4 Co dero2ss 4tqCe Cas wared a_d qo2q qCe Cared a_ qo ca5erse a_qeres a_d a_d qt5e sqerase_dt_g Ce qCer,.wore qo 5e o2rd qo Cas artqqere a_d qCtq sor altde Cer Co2d Caq wt_g qCe a_d qCe..o4 Ca sear aq qortqCt_ge Cad so2d qCer st_d Ct_ Caq ar o_d qo sCe a_d qCar wer o_ st5 qtq Ct5 o4 Co2 Cer wt_g oqCe Cer a_des wtqCe Ct_g qa_d a_d qo qore qore ald Ce wo_g qa qCor, qCe qCe wCad qo qo sCe wased qCe ardt_g qte_de qCare st5tde, Cts t_ed a_d o_ ard allaqed st_qe_ aqe so2q Ce.so2d Ctq Ct_ wea_d o2qCereded a_d a_eder a_d a_e qCa_d sted sCtd Cer.a_gaq a_ Ct5ed, ald sCe a_d o4 qCas Ct_s wCaqe_ qt_celerstqe \n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 2000, prime='Anna', top_k=5, cuda=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
