{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data and Inspect Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(text) =1,985,223\n",
      "\n",
      "\n",
      "text[0:100]=\n",
      "Chapter 1\n",
      "\n",
      "\n",
      "Happy families are all alike; every un\n"
     ]
    }
   ],
   "source": [
    "print('len(text) ={:,}'.format(len(text)))\n",
    "print('\\n')\n",
    "print('text[0:100]=\\n{}'.format(text[0:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Text Encoder and Encode Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = tuple(set(text))\n",
    "idx2word = dict(enumerate(chars))\n",
    "word2idx = {word: idx for idx, word in idx2word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(word2idx)=83\n"
     ]
    }
   ],
   "source": [
    "print('len(word2idx)={}'.format(len(word2idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2idx={'z': 0, 'Y': 1, 'r': 2, '@': 3, 'i': 4, 'W': 5, 'B': 6, '4': 7, '7': 8, 'S': 9, 'q': 10, 'x': 11, 'R': 12, 's': 13, 'o': 14, \"'\": 15, 'G': 16, '9': 17, 'J': 18, ';': 19, ',': 20, 'N': 21, 'K': 22, 'T': 23, '0': 24, 'l': 25, ' ': 26, 'k': 27, 'm': 28, 'a': 29, 'E': 30, 'F': 31, '\"': 32, '8': 33, 'p': 34, 'w': 35, 'e': 36, 'g': 37, '1': 38, 'h': 39, 'u': 40, 'P': 41, 'L': 42, 'n': 43, 'A': 44, 'I': 45, 'Z': 46, 'U': 47, 'Q': 48, '*': 49, '3': 50, 'y': 51, '.': 52, 'H': 53, 'V': 54, '&': 55, '!': 56, '$': 57, '5': 58, 'd': 59, 'j': 60, 'c': 61, '2': 62, 't': 63, '`': 64, 'C': 65, '6': 66, 'O': 67, 'M': 68, '\\n': 69, '_': 70, '-': 71, 'X': 72, ')': 73, ':': 74, 'f': 75, 'b': 76, 'D': 77, '(': 78, '?': 79, 'v': 80, '/': 81, '%': 82}\n",
      "text[0:50]=\n",
      "Chapter 1\n",
      "\n",
      "\n",
      "Happy families are all alike; every un\n",
      "\n",
      "\n",
      "encoded[0:50]=\n",
      "[65 39 29 34 63 36  2 26 38 69 69 69 53 29 34 34 51 26 75 29 28  4 25  4 36\n",
      " 13 26 29  2 36 26 29 25 25 26 29 25  4 27 36 19 26 36 80 36  2 51 26 40 43]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "encoded = np.array([word2idx[w] for w in text])\n",
    "\n",
    "print('word2idx={}'.format(word2idx))\n",
    "print('text[0:50]=\\n{}'.format(text[0:50]))\n",
    "print('\\n')\n",
    "print('encoded[0:50]=\\n{}'.format(encoded[0:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_batches(arr, batch_size, seq_length):\n",
    "    n_seq = len(arr) // (batch_size * seq_length)\n",
    "    arr = arr[:batch_size * seq_length * n_seq]\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        \n",
    "        x = arr[:, n:n+seq_length]\n",
    "        y = np.zeros_like(x)\n",
    "\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Batch Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:\n",
      "\tseq 1\n",
      "\t\tx:\t[29 37 43  4 75  4 61 36 43 63 19 26 63 39  4  2 63 51 71 36]\n",
      "\t\ty:\t   [37 43  4 75  4 61 36 43 63 19 26 63 39  4  2 63 51 71 36  4]\n",
      "\t\tx_word:\t'agnificent; thirty-e'\n",
      "\t\ty_word:\t 'gnificent; thirty-ei'\n",
      "\tseq 2\n",
      "\t\tx:\t[36 26 63 39 29 43 27 36 59 26 42 36 80  4 43 26 29 43 59 26]\n",
      "\t\ty:\t   [26 63 39 29 43 27 36 59 26 42 36 80  4 43 26 29 43 59 26 35]\n",
      "\t\tx_word:\t'e thanked Levin and '\n",
      "\t\ty_word:\t ' thanked Levin and w'\n",
      "Batch 1:\n",
      "\tseq 0\n",
      "\t\tx:\t[28  4 25  4 36 13 26 29  2 36 26 29 25 25 26 29 25  4 27 36]\n",
      "\t\ty:\t   [ 4 25  4 36 13 26 29  2 36 26 29 25 25 26 29 25  4 27 36 19]\n",
      "\t\tx_word:\t'milies are all alike'\n",
      "\t\ty_word:\t 'ilies are all alike;'\n",
      "Batch 2:\n",
      "\tseq 0\n",
      "\t\tx:\t[19 26 36 80 36  2 51 26 40 43 39 29 34 34 51 26 75 29 28  4]\n",
      "\t\ty:\t   [26 36 80 36  2 51 26 40 43 39 29 34 34 51 26 75 29 28  4 25]\n",
      "\t\tx_word:\t'; every unhappy fami'\n",
      "\t\ty_word:\t ' every unhappy famil'\n",
      "\tseq 1\n",
      "\t\tx:\t[26 13 63  2 29  4 37 39 63 26 29 35 29 51 20 26 29 43 59 26]\n",
      "\t\ty:\t   [13 63  2 29  4 37 39 63 26 29 35 29 51 20 26 29 43 59 26 63]\n",
      "\t\tx_word:\t' straight away, and '\n",
      "\t\ty_word:\t 'straight away, and t'\n",
      "\tseq 2\n",
      "\t\tx:\t[43 26 35 36 43 63 26  4 43 63 14 26 63 39 36 26 27  4 63 61]\n",
      "\t\ty:\t   [26 35 36 43 63 26  4 43 63 14 26 63 39 36 26 27  4 63 61 39]\n",
      "\t\tx_word:\t'n went into the kitc'\n",
      "\t\ty_word:\t ' went into the kitch'\n"
     ]
    }
   ],
   "source": [
    "for index, (x, y) in enumerate(get_batches(encoded, 5, 20)):\n",
    "    if index == 3:\n",
    "        break\n",
    "        \n",
    "    print('Batch {}:'.format(index))\n",
    "        \n",
    "    for i_seq, (x_seq, y_seq) in enumerate(zip(x, y)):\n",
    "        \n",
    "        # Don't show newline because of readability\n",
    "        if word2idx['\\n'] in x_seq:\n",
    "            continue \n",
    "            \n",
    "        if i_seq == 3:\n",
    "            break\n",
    "            \n",
    "        print('\\tseq {}'.format(i_seq))\n",
    "        print('\\t\\tx:\\t{}'.format(x_seq))\n",
    "        print('\\t\\ty:\\t   {}'.format(y_seq))\n",
    "        \n",
    "        x_word = [idx2word[idx] for idx in x_seq]\n",
    "        y_word = [idx2word[idx] for idx in y_seq]\n",
    "        print('\\t\\tx_word:\\t\\'{}\\''.format(''.join(x_word)))\n",
    "        print('\\t\\ty_word:\\t \\'{}\\''.format(''.join(y_word)))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define One-Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, tokens, n_steps=100, n_hidden=256, n_layers=2,\n",
    "                        drop_prob=0.5, lr=0.001):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(set(self.chars)))\n",
    "        self.char2int = {c: i for i, c in self.int2char.items()}\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=len(self.chars),\n",
    "                           hidden_size=n_hidden,\n",
    "                           num_layers=n_layers,\n",
    "                           dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hc):\n",
    "        \n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        x = x.view(x.size()[0]*x.size()[1], self.n_hidden)\n",
    "        \n",
    "        ## TODO: put x through the fully-connected layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x, (h, c)\n",
    "    \n",
    "    \n",
    "    def predict(self, char, h=None, cuda=False, top_k=None):\n",
    "        \n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "        else:\n",
    "            self.cpu()\n",
    "            \n",
    "        if h is None:\n",
    "            h = self.init_hidden(1)\n",
    "            \n",
    "        x = np.array([[self.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(self.chars))\n",
    "        \n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "            \n",
    "        h = tuple([each.data for each in h])\n",
    "        out, h = self.forward(inputs, h)\n",
    "        \n",
    "        p = F.softmax(out, dim=1).data\n",
    "        \n",
    "        if cuda:\n",
    "            p = p.cpu()\n",
    "            \n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(self.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "            \n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "            \n",
    "        return self.int2char[char], h\n",
    "        \n",
    "   \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        \n",
    "        self.fc.bias.data.fill_(0)\n",
    "        \n",
    "        self.fc.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        return (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, \n",
    "          lr=0.001, clip=5, val_frac=0.1, cuda=False, print_every=10):\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    # define optimizer\n",
    "    opt = optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    # define loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training set and validation set\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    \n",
    "    n_chars = len(net.chars)\n",
    "    \n",
    "    # start training\n",
    "    for e in range(epochs):\n",
    "        counter = 0\n",
    "        \n",
    "        # reset weight\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        # get batch of encoded data\n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            \n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "            # move tensor from cpu to gpu\n",
    "            if cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                \n",
    "                \n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "                \n",
    "            net.zero_grad()\n",
    "    \n",
    "            output, h = net.forward(inputs, h)\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "                \n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                \n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    \n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if cuda:\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                        \n",
    "                        \n",
    "                    output, val_h = net.forward(inputs, val_h)\n",
    "                    \n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length))\n",
    "                    \n",
    "                    val_losses.append(val_loss.item())\n",
    "                    \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
    "            inputs = inputs.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'net' in locals():\n",
    "    del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = CharRNN(chars, n_hidden=512, n_layers=2)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10... Step: 10... Loss: 1.5249... Val Loss: 1.6193\n",
      "Epoch: 1/10... Step: 20... Loss: 1.5204... Val Loss: 1.6094\n",
      "Epoch: 1/10... Step: 30... Loss: 1.5000... Val Loss: 1.6003\n",
      "Epoch: 1/10... Step: 40... Loss: 1.4716... Val Loss: 1.5969\n",
      "Epoch: 1/10... Step: 50... Loss: 1.4979... Val Loss: 1.5931\n",
      "Epoch: 1/10... Step: 60... Loss: 1.4352... Val Loss: 1.5884\n",
      "Epoch: 1/10... Step: 70... Loss: 1.4355... Val Loss: 1.5897\n",
      "Epoch: 1/10... Step: 80... Loss: 1.4260... Val Loss: 1.5922\n",
      "Epoch: 1/10... Step: 90... Loss: 1.4545... Val Loss: 1.5826\n",
      "Epoch: 1/10... Step: 100... Loss: 1.4428... Val Loss: 1.5872\n",
      "Epoch: 1/10... Step: 110... Loss: 1.4344... Val Loss: 1.5771\n",
      "Epoch: 1/10... Step: 120... Loss: 1.4205... Val Loss: 1.5823\n",
      "Epoch: 1/10... Step: 130... Loss: 1.4584... Val Loss: 1.5685\n",
      "Epoch: 2/10... Step: 10... Loss: 1.4654... Val Loss: 1.5714\n",
      "Epoch: 2/10... Step: 20... Loss: 1.4809... Val Loss: 1.5662\n",
      "Epoch: 2/10... Step: 30... Loss: 1.4492... Val Loss: 1.5612\n",
      "Epoch: 2/10... Step: 40... Loss: 1.4261... Val Loss: 1.5644\n",
      "Epoch: 2/10... Step: 50... Loss: 1.4551... Val Loss: 1.5645\n",
      "Epoch: 2/10... Step: 60... Loss: 1.3923... Val Loss: 1.5525\n",
      "Epoch: 2/10... Step: 70... Loss: 1.4064... Val Loss: 1.5573\n",
      "Epoch: 2/10... Step: 80... Loss: 1.3933... Val Loss: 1.5557\n",
      "Epoch: 2/10... Step: 90... Loss: 1.4098... Val Loss: 1.5583\n",
      "Epoch: 2/10... Step: 100... Loss: 1.4064... Val Loss: 1.5508\n",
      "Epoch: 2/10... Step: 110... Loss: 1.3959... Val Loss: 1.5498\n",
      "Epoch: 2/10... Step: 120... Loss: 1.3789... Val Loss: 1.5494\n",
      "Epoch: 2/10... Step: 130... Loss: 1.4187... Val Loss: 1.5509\n",
      "Epoch: 3/10... Step: 10... Loss: 1.4295... Val Loss: 1.5484\n",
      "Epoch: 3/10... Step: 20... Loss: 1.4363... Val Loss: 1.5412\n",
      "Epoch: 3/10... Step: 30... Loss: 1.4256... Val Loss: 1.5362\n",
      "Epoch: 3/10... Step: 40... Loss: 1.3908... Val Loss: 1.5379\n",
      "Epoch: 3/10... Step: 50... Loss: 1.4103... Val Loss: 1.5460\n",
      "Epoch: 3/10... Step: 60... Loss: 1.3458... Val Loss: 1.5419\n",
      "Epoch: 3/10... Step: 70... Loss: 1.3589... Val Loss: 1.5347\n",
      "Epoch: 3/10... Step: 80... Loss: 1.3556... Val Loss: 1.5341\n",
      "Epoch: 3/10... Step: 90... Loss: 1.3763... Val Loss: 1.5344\n",
      "Epoch: 3/10... Step: 100... Loss: 1.3766... Val Loss: 1.5301\n",
      "Epoch: 3/10... Step: 110... Loss: 1.3661... Val Loss: 1.5292\n",
      "Epoch: 3/10... Step: 120... Loss: 1.3476... Val Loss: 1.5313\n",
      "Epoch: 3/10... Step: 130... Loss: 1.3796... Val Loss: 1.5236\n",
      "Epoch: 4/10... Step: 10... Loss: 1.4030... Val Loss: 1.5294\n",
      "Epoch: 4/10... Step: 20... Loss: 1.4148... Val Loss: 1.5197\n",
      "Epoch: 4/10... Step: 30... Loss: 1.3897... Val Loss: 1.5136\n",
      "Epoch: 4/10... Step: 40... Loss: 1.3628... Val Loss: 1.5168\n",
      "Epoch: 4/10... Step: 50... Loss: 1.3926... Val Loss: 1.5138\n",
      "Epoch: 4/10... Step: 60... Loss: 1.3282... Val Loss: 1.5107\n",
      "Epoch: 4/10... Step: 70... Loss: 1.3481... Val Loss: 1.5062\n",
      "Epoch: 4/10... Step: 80... Loss: 1.3297... Val Loss: 1.5056\n",
      "Epoch: 4/10... Step: 90... Loss: 1.3510... Val Loss: 1.5091\n",
      "Epoch: 4/10... Step: 100... Loss: 1.3423... Val Loss: 1.5072\n",
      "Epoch: 4/10... Step: 110... Loss: 1.3311... Val Loss: 1.5027\n",
      "Epoch: 4/10... Step: 120... Loss: 1.3187... Val Loss: 1.5052\n",
      "Epoch: 4/10... Step: 130... Loss: 1.3595... Val Loss: 1.4989\n",
      "Epoch: 5/10... Step: 10... Loss: 1.3767... Val Loss: 1.5065\n",
      "Epoch: 5/10... Step: 20... Loss: 1.3843... Val Loss: 1.4982\n",
      "Epoch: 5/10... Step: 30... Loss: 1.3658... Val Loss: 1.4992\n",
      "Epoch: 5/10... Step: 40... Loss: 1.3344... Val Loss: 1.5040\n",
      "Epoch: 5/10... Step: 50... Loss: 1.3575... Val Loss: 1.5008\n",
      "Epoch: 5/10... Step: 60... Loss: 1.3054... Val Loss: 1.4949\n",
      "Epoch: 5/10... Step: 70... Loss: 1.3196... Val Loss: 1.4945\n",
      "Epoch: 5/10... Step: 80... Loss: 1.3002... Val Loss: 1.4951\n",
      "Epoch: 5/10... Step: 90... Loss: 1.3340... Val Loss: 1.4956\n",
      "Epoch: 5/10... Step: 100... Loss: 1.3274... Val Loss: 1.4942\n",
      "Epoch: 5/10... Step: 110... Loss: 1.3044... Val Loss: 1.5017\n",
      "Epoch: 5/10... Step: 120... Loss: 1.2906... Val Loss: 1.4846\n",
      "Epoch: 5/10... Step: 130... Loss: 1.3309... Val Loss: 1.4814\n",
      "Epoch: 6/10... Step: 10... Loss: 1.3562... Val Loss: 1.4874\n",
      "Epoch: 6/10... Step: 20... Loss: 1.3741... Val Loss: 1.4864\n",
      "Epoch: 6/10... Step: 30... Loss: 1.3518... Val Loss: 1.4816\n",
      "Epoch: 6/10... Step: 40... Loss: 1.3312... Val Loss: 1.4836\n",
      "Epoch: 6/10... Step: 50... Loss: 1.3246... Val Loss: 1.4862\n",
      "Epoch: 6/10... Step: 60... Loss: 1.2784... Val Loss: 1.4784\n",
      "Epoch: 6/10... Step: 70... Loss: 1.2937... Val Loss: 1.4734\n",
      "Epoch: 6/10... Step: 80... Loss: 1.2773... Val Loss: 1.4767\n",
      "Epoch: 6/10... Step: 90... Loss: 1.3074... Val Loss: 1.4811\n",
      "Epoch: 6/10... Step: 100... Loss: 1.2966... Val Loss: 1.4697\n",
      "Epoch: 6/10... Step: 110... Loss: 1.2867... Val Loss: 1.4800\n",
      "Epoch: 6/10... Step: 120... Loss: 1.2794... Val Loss: 1.4864\n",
      "Epoch: 6/10... Step: 130... Loss: 1.3149... Val Loss: 1.4708\n",
      "Epoch: 7/10... Step: 10... Loss: 1.3328... Val Loss: 1.4747\n",
      "Epoch: 7/10... Step: 20... Loss: 1.3422... Val Loss: 1.4732\n",
      "Epoch: 7/10... Step: 30... Loss: 1.3208... Val Loss: 1.4709\n",
      "Epoch: 7/10... Step: 40... Loss: 1.3031... Val Loss: 1.4688\n",
      "Epoch: 7/10... Step: 50... Loss: 1.3189... Val Loss: 1.4682\n",
      "Epoch: 7/10... Step: 60... Loss: 1.2508... Val Loss: 1.4741\n",
      "Epoch: 7/10... Step: 70... Loss: 1.2763... Val Loss: 1.4644\n",
      "Epoch: 7/10... Step: 80... Loss: 1.2654... Val Loss: 1.4620\n",
      "Epoch: 7/10... Step: 90... Loss: 1.2890... Val Loss: 1.4668\n",
      "Epoch: 7/10... Step: 100... Loss: 1.2900... Val Loss: 1.4687\n",
      "Epoch: 7/10... Step: 110... Loss: 1.2752... Val Loss: 1.4596\n",
      "Epoch: 7/10... Step: 120... Loss: 1.2591... Val Loss: 1.4571\n",
      "Epoch: 7/10... Step: 130... Loss: 1.2859... Val Loss: 1.4576\n",
      "Epoch: 8/10... Step: 10... Loss: 1.3177... Val Loss: 1.4600\n",
      "Epoch: 8/10... Step: 20... Loss: 1.3134... Val Loss: 1.4504\n",
      "Epoch: 8/10... Step: 30... Loss: 1.3191... Val Loss: 1.4590\n",
      "Epoch: 8/10... Step: 40... Loss: 1.2785... Val Loss: 1.4628\n",
      "Epoch: 8/10... Step: 50... Loss: 1.3010... Val Loss: 1.4546\n",
      "Epoch: 8/10... Step: 60... Loss: 1.2395... Val Loss: 1.4630\n",
      "Epoch: 8/10... Step: 70... Loss: 1.2571... Val Loss: 1.4667\n",
      "Epoch: 8/10... Step: 80... Loss: 1.2472... Val Loss: 1.4514\n",
      "Epoch: 8/10... Step: 90... Loss: 1.2711... Val Loss: 1.4527\n",
      "Epoch: 8/10... Step: 100... Loss: 1.2524... Val Loss: 1.4544\n",
      "Epoch: 8/10... Step: 110... Loss: 1.2600... Val Loss: 1.4473\n",
      "Epoch: 8/10... Step: 120... Loss: 1.2460... Val Loss: 1.4401\n",
      "Epoch: 8/10... Step: 130... Loss: 1.2694... Val Loss: 1.4414\n",
      "Epoch: 9/10... Step: 10... Loss: 1.2923... Val Loss: 1.4427\n",
      "Epoch: 9/10... Step: 20... Loss: 1.3100... Val Loss: 1.4456\n",
      "Epoch: 9/10... Step: 30... Loss: 1.3049... Val Loss: 1.4377\n",
      "Epoch: 9/10... Step: 40... Loss: 1.2596... Val Loss: 1.4462\n",
      "Epoch: 9/10... Step: 50... Loss: 1.2865... Val Loss: 1.4465\n",
      "Epoch: 9/10... Step: 60... Loss: 1.2220... Val Loss: 1.4481\n",
      "Epoch: 9/10... Step: 70... Loss: 1.2485... Val Loss: 1.4423\n",
      "Epoch: 9/10... Step: 80... Loss: 1.2348... Val Loss: 1.4505\n",
      "Epoch: 9/10... Step: 90... Loss: 1.2524... Val Loss: 1.4400\n",
      "Epoch: 9/10... Step: 100... Loss: 1.2482... Val Loss: 1.4402\n",
      "Epoch: 9/10... Step: 110... Loss: 1.2339... Val Loss: 1.4370\n",
      "Epoch: 9/10... Step: 120... Loss: 1.2288... Val Loss: 1.4435\n",
      "Epoch: 9/10... Step: 130... Loss: 1.2570... Val Loss: 1.4429\n",
      "Epoch: 10/10... Step: 10... Loss: 1.2755... Val Loss: 1.4397\n",
      "Epoch: 10/10... Step: 20... Loss: 1.2926... Val Loss: 1.4281\n",
      "Epoch: 10/10... Step: 30... Loss: 1.2816... Val Loss: 1.4364\n",
      "Epoch: 10/10... Step: 40... Loss: 1.2588... Val Loss: 1.4458\n",
      "Epoch: 10/10... Step: 50... Loss: 1.2618... Val Loss: 1.4353\n",
      "Epoch: 10/10... Step: 60... Loss: 1.2162... Val Loss: 1.4309\n",
      "Epoch: 10/10... Step: 70... Loss: 1.2309... Val Loss: 1.4318\n",
      "Epoch: 10/10... Step: 80... Loss: 1.2189... Val Loss: 1.4295\n",
      "Epoch: 10/10... Step: 90... Loss: 1.2367... Val Loss: 1.4308\n",
      "Epoch: 10/10... Step: 100... Loss: 1.2293... Val Loss: 1.4402\n",
      "Epoch: 10/10... Step: 110... Loss: 1.2190... Val Loss: 1.4314\n",
      "Epoch: 10/10... Step: 120... Loss: 1.2071... Val Loss: 1.4295\n",
      "Epoch: 10/10... Step: 130... Loss: 1.2358... Val Loss: 1.4236\n",
      "Execution Time:0:06:27.578469\n",
      "Time per Epoch=0:00:38.757847\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "batch_size, seq_length = 128, 100\n",
    "epochs = 10\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "# you may change cuda to True if you plan on using a GPU!\n",
    "# also, if you do, please INCREASE the epochs to 25\n",
    "train(net, encoded, epochs=epochs, batch_size=batch_size, seq_length=seq_length, \n",
    "      lr=0.001, cuda=True, print_every=10)\n",
    "\n",
    "end = datetime.now()\n",
    "print('Execution Time:{}'.format(end - start))\n",
    "print('Time per Epoch={}'.format((end-start)/epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'lstm_{}_epochs.net'.format(epochs)\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "             'n_layers': net.n_layers,\n",
    "             'state_dict': net.state_dict(),\n",
    "             'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None, cuda=False):\n",
    "        \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = net.predict(ch, h, cuda=cuda, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = net.predict(chars[-1], h, cuda=cuda, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna\n",
      "as he had been taking them. He was not satisfaction to him when\n",
      "the peasants and a cheers and his cannocily had\n",
      "studied with his broad of taking, as\n",
      "though the priest had said that he was told her and the same importance\n",
      "was a great coultry were to be distinctly, so took on a left weal heart. He was not his early, to himself it he\n",
      "could say shooting there and the continual trivial who came before the talk, but the most things\n",
      "were not already been sure the person in the most side as always and so me that it's not\n",
      "silving would have to dive man an official servant the chair, to say a merchant to be, but I want to\n",
      "see the trunk,\" said Alexey Alexandrovitch. She stopped to her some song\n",
      "of sollet of a plant fall of the same the time at the conversation, that\n",
      "he was clutching her as the came in the brother with the master, and that\n",
      "stinge thought harring for his world, the steps which he could not thrang on her, and\n",
      "went into the convinces of her more time there had not been\n",
      "talking to him from the country and an one should\n",
      "hand to him in that in his eyes and their positions with its all\n",
      "something of the settles to step in the court thought he could not see him. He\n",
      "had not so servoute, and saying that served, the sight of this weakness, her\n",
      "feel, and the same door at his would been herself towards them\n",
      "to his best that she was tearing her finger and a strittering things, with his\n",
      "subject a secretary of the contrary, he had thought he seemed to\n",
      "see him a song of the children and the mudder short of healing a money\n",
      "and the fatherish of the menthing stand of the same artistivation of\n",
      "her hand.\n",
      "\n",
      "\"Yes, yes in society!\" she said, with a little contrad, he saw the present\n",
      "of agreement than surprised in to be of her time\n",
      "went on to the cord and any hotel he had something were so satisfied to them to\n",
      "him. She had to speak of the stirk of the completies with the penine\n",
      "as all the sense of her mouth were she tried, soon to him, he had a\n",
      "standance, though he would not his fell in h\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 2000, prime='Anna', top_k=5, cuda=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
